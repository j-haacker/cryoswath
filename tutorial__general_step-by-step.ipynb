{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step-by-step tutorial\n",
    "\n",
    "**Scope:**  \n",
    "\n",
    "This tutorial shows how to use cryoswath to process CryoSat-2 SARIn L1b\n",
    "data to gridded trends of elevation change using the default setting.\n",
    "All processing steps are triggered manually to show what usually happens\n",
    "in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: if you haven't done so, install the packages listed\n",
    "in [`../requirements.txt`](../requirements.txt). The required reference\n",
    "DEM and shape files for this tutorial are provided, however, note that\n",
    "this will likely not be sufficient for your own research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use Barnes Ice Cap in the Southern Canadian Arctic as study region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_aux_data_path = os.path.join(\"..\", \"data\", \"tutorials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_file_path = os.path.join(tutorial_aux_data_path, \"arcticdem_mosaic_100m_v4.1_dem__excerpt_barnes-ice-cap.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load barnes ice cap outline\n",
    "barnes_shp = gpd.read_feather(os.path.join(tutorial_aux_data_path, \"barnes_ice_cap.feather\")).unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, CryoSat-2 SARIn ground tracks, that pass in some defined period\n",
    "close to Barnes Ice Cap, will be loaded. You can best reduce the\n",
    "downloaded and processed amount of data, by editing the start and end\n",
    "dates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the package available/from search path\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from cryoswath import misc  # this module contains helper functions\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_tracks = misc.load_cs_ground_tracks(\n",
    "    barnes_shp, \"2020-09\", \"2023-09\", # request region and period\n",
    "    buffer_period_by=relativedelta(months=1), # include previous and following months\n",
    "    buffer_region_by=5_000) # include basin margins\n",
    "print(\"number of tracks:\", example_tracks.shape[0], \"\\n last five:\", example_tracks.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pandas.Series` contains the CryoSat-2 tracks. However, what we are\n",
    "more interested in, here, is the Index of the Series which can be used\n",
    "to download the associated L1b files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You could as well immediately pass the region definitions to the\n",
    "download function - in fact, you could jump all the steps below. This\n",
    "detour is only for the purpose of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check whether L1b files are present and download if they aren't\n",
    "# l1b.download_wrapper(track_idx=example_tracks.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level L1b to L2 processing\n",
    "\n",
    "L1b describes a data stage where the data have undergone some\n",
    "preprocessing but relate only indirectly to the observable world.\n",
    "\n",
    "L2 refers to data that describe physical \"real-world\" properties. This\n",
    "will mainly be the surface elevation at various locations, here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first of the tracks.\n",
    "\n",
    "You can load the L1b data by either using the index of the concerned\n",
    "track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = l1b.L1bData.from_id(example_tracks.index[0], drop_outside=barnes_shp, smooth_phase_difference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, first groups of continuous samples have to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.tag_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the groups identified, you can \"unwrap\" the phase difference.\n",
    "(Unwrapping is like realizing that a series of time stamps: 11 pm, 12\n",
    "pm, 1 am extends over two days.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.unwrap_phase_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, you can have the reference elevations calculated for each\n",
    "apparent echo origin. For this, obviously, one need a reference\n",
    "elevation dataset. For this tutorial, it is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_file_path = os.path.join(tutorial_aux_data_path, \"arcticdem_mosaic_100m_v4.1_dem__excerpt_barnes-ice-cap.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_ambiguous_reference_elevation(dem_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cryoswath frequently uses the elevation difference of the echo origin\n",
    "with regard to the reference dataset. Below, this value is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_elev_diff_to_ref()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each group, the best fitting of the ambiguous echo origins\n",
    "is found based on a statistical measure. Below, the default is made\n",
    "explicit. As long as you prefer the default statistic, you don't need to\n",
    "pass the argument. However, it shows how you could define any custom\n",
    "measure you deem appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation\n",
    "def the_default(elev_diff):\n",
    "    return np.argmin(np.abs(np.median(elev_diff, axis=0))**2+median_abs_deviation(elev_diff, axis=0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_best_fit_phase_index(best_column=the_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the result:  \n",
    "*(The points in different shades of gray indicate other mathematically\n",
    "valid echo origins.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath.test_plots.waveform import dem_transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_transect(example_l1b_data.isel(time_20_ku=[len(example_l1b_data.time_20_ku)//2]), dem_file_name_or_path=dem_file_path, selected_phase_only=False)#, ax=ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final remarks L1b to L2**\n",
    "\n",
    "This were the most important steps in the L1b to L2 processing. Usually,\n",
    "you will rather use shorthands to have the steps done above\n",
    "automatically. However, if you have reasons to do so, this is how you\n",
    "can trigger all the major steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 to L3 processing\n",
    "\n",
    "L3 data features slots for a set of locations and time steps. For our\n",
    "case, L3 data refers to gridded monthly elevation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, all the tracks previously are processed - now, using a shorthand.\n",
    "\n",
    "The arguments, usually, are not necessary. Here, they set the caching\n",
    "location to the tutorial directory, confine the processed data to the\n",
    "study region, pass the elevation model, set a threshold for the maximum\n",
    "disagreement wrt. the reference DEM, prohibit parallelism (often a good\n",
    "idea on notebooks with limited working memory), define a coordinate\n",
    "reference system (CRS) for the output locations, and make explicit that\n",
    "all data is processed, even if it is present. The last option, you may\n",
    "want to disable if you know that the tracks have not been processed with\n",
    "different settings (e.g., you run the cell for the second time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:* should you do the l1b to l2 processing below in multiple\n",
    "        sessions (takes about 30 min in total), the returned\n",
    "        `swath_data, poca_data` will only contain the part processed in\n",
    "        the last run. Should you want to load all data, use the\n",
    "        commented cell below (only works once you've run the first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "swath_data, poca_data = l2.from_id(example_tracks.index, cache_fullname=os.path.join(tmp_path, \"tmp_l2_cache\"), drop_outside=barnes_shp, dem_file_name_or_path=dem_file_path, max_elev_diff=150, cores=1, crs=3413, reprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a few minutes (2-3 min on my notebook)\n",
    "# empty_shape = gpd.GeoSeries().union_all(\"coverage\")\n",
    "# swath_data, poca_data = l2.from_id(example_tracks.index, reprocess=False, \n",
    "#                                    save_or_return=\"return\", # only return data (do not save to disk)\n",
    "#                                    drop_outside=empty_shape, # indirectly: do not process any files\n",
    "#                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take a look at the data by producing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "swath_data.h_diff.plot.hist(label=f\"swath, Σ={len(swath_data):_}\", bins=100, density=True)\n",
    "poca_data.h_diff.plot.hist(label=f\"POCA, Σ={len(poca_data):_}\", bins=100, density=True, ax=plt.gca(), alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aggregate the point data into a regular grid. You will\n",
    "recognize most of the arguments from the previous steps. However, I\n",
    "would like to highlight the `agg_func_and_meta` argument. This allows to\n",
    "pass a custom aggregation function. Be careful modifying the aggregates\n",
    "names, because as of now (Feb 2025) those names are hardcoded in other\n",
    "functions. So feel free to play around, but keep the names if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_iqr_cnt(data):\n",
    "    quartiles = np.quantile(data, [.25, .5, .75])\n",
    "    return pd.DataFrame([[\n",
    "        quartiles[1],  # the median\n",
    "        quartiles[2]-quartiles[0],  # the inter-quartile range\n",
    "        len(data)  # the data count\n",
    "    ]], columns=[\"_median\", \"_iqr\", \"_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a few minutes (3-4 min at 4 GHz)\n",
    "gridded_elev_diffs = l3.build_dataset(\n",
    "    barnes_shp, start_datetime=\"2020-09\", end_datetime=\"2023-09\",\n",
    "    cache_filename=\"tmp_l2_cache\",\n",
    "    drop_outside=barnes_shp,\n",
    "    buffer_region_by=5_000,\n",
    "    crs=3413,\n",
    "    reprocess=False,\n",
    "    timestep_months = 1,  # temporal resolution\n",
    "    window_ntimesteps = 3,  # aggregates multiple time steps (moving window)\n",
    "    spatial_res_meter = 500,  # size of the grid cells\n",
    "    # below, you need to disclose the returned format\n",
    "    agg_func_and_meta = (med_iqr_cnt, {\"_median\": \"f4\", \"_iqr\": \"f4\", \"_count\": \"i4\"}),\n",
    ").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you see warnings about dumped data in the output above, don't\n",
    "worry. This usually concerns only data outside of the glaciers and it\n",
    "can be recovered using a helper function that collects the dumps. I'll\n",
    "fix it (#37) after finishing the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 to L4 processing\n",
    "\n",
    "L4 data may contain inferred information. In our case, that concerns\n",
    "void filling and calculating change rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do a rudimentary data clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_elev_diffs[\"_median\"] = gridded_elev_diffs._median.where(gridded_elev_diffs._count>3).where(gridded_elev_diffs._iqr<30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, the elevation change rate can be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_fit_results = l4.fit_trend__seasons_removed(gridded_elev_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before later we can fill the voids, we should exclude outliers to\n",
    "prevent them from distorting the data distribution. Here, I do so by\n",
    "requiring the trend variance to be less than 2 m²/yr² and the amplitude\n",
    "variances of the annual and semi-annual cycle to be smaller than 100 m².\n",
    "This corresponds roughly to limiting the trend and amplitude 2-sigma\n",
    "uncertainties to 3 m/yr and 20 m, respectively.\n",
    "\n",
    "For clarity, below the new \"trend\" and \"trend_std\" variables are\n",
    "defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(\n",
    "    gridded_fit_results.curvefit_covariance.sel(cov_i=\"trend\", cov_j=\"trend\") < 2,\n",
    "    np.logical_and(gridded_fit_results.curvefit_covariance.sel(cov_i=\"amp_yearly\", cov_j=\"amp_yearly\") < 100,\n",
    "                    gridded_fit_results.curvefit_covariance.sel(cov_i=\"amp_semiyr\", cov_j=\"amp_semiyr\") < 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = gridded_fit_results.where(mask).sel(param=\"trend\", cov_i=\"trend\", cov_j=\"trend\")\n",
    "gridded_trends[\"trend\"] = gridded_trends.curvefit_coefficients\n",
    "gridded_trends[\"trend_std\"] = gridded_trends.curvefit_covariance**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, then, ensure that there is no missing grid cells. This used to occur\n",
    "before I moved to Zarr for regions with larger gaps between glaciers. If\n",
    "grid cells are missing, this messes up the apparent resolution as seen\n",
    "by `rioxarray` that is used in the background.\n",
    "\n",
    "Further, we cut the data that is outside of the glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "barnes_GeoDataFrame_SRID3413 = gpd.read_feather(os.path.join(tutorial_aux_data_path, \"barnes_ice_cap.feather\")).to_crs(gridded_trends.rio.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.fill_missing_coords(gridded_trends, *barnes_GeoDataFrame_SRID3413.total_bounds)\n",
    "gridded_trends = gridded_trends.rio.clip(barnes_GeoDataFrame_SRID3413.make_valid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we append the elevation of a reference DEM and assign the grid\n",
    "cells to basins. Actually, here we do *not* consider the basins, but\n",
    "treat the entire ice cap collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.append_elevation_reference(gridded_trends.rio.clip(barnes_GeoDataFrame_SRID3413.geometry), dem_file_name_or_path=dem_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.fill_voids(gridded_trends[[\"trend\", \"trend_std\", \"ref_elev\"]],\n",
    "                               main_var=\"trend\",\n",
    "                               error=\"trend_std\",\n",
    "                               elev=\"ref_elev\",\n",
    "                               basin_shapes=barnes_GeoDataFrame_SRID3413,\n",
    "                               per=tuple(), outlier_replace=True, outlier_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends.trend.T.plot(robust=True, cmap=\"RdYlBu\")\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regarding the above figure*  \n",
    "There are artifacts, showing as patches of strong elevation gains and\n",
    "losses (blue and red) at higher elevations along the ice divide at the\n",
    "center of the ice cap. You could use the `dem_transect` plotting\n",
    "function from above to investigate their origin. Some issues are known\n",
    "to especially affect data covering only less than 4 years and to occur\n",
    "around summits (web-search: systematic error cryosat swath)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series\n",
    "\n",
    "**coming soon**\n",
    "\n",
    "Calculating time series works in analogy to the above. However, this\n",
    "tutorial works on a RGI \"complex\", here, the Barnes Ice Cap, while I\n",
    "usually work on \"basins\", i.e., constituting parts of complexes. I need\n",
    "to adapt the code a bit to work on complexes as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryoswath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
