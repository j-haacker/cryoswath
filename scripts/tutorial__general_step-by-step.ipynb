{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Step-by-step tutorial\n",
    "\n",
    "**Scope:**  \n",
    "\n",
    "This tutorial shows how to use cryoswath to process CryoSat-2 SARIn L1b\n",
    "data to gridded trends of elevation change using the default setting.\n",
    "All processing steps are triggered manually to show what usually happens\n",
    "in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: if you haven't done so, install the packages listed\n",
    "in [`../requirements.txt`](../requirements.txt). The required reference\n",
    "DEM and shape files for this tutorial are provided, however, note that\n",
    "this will likely not be sufficient for your own research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use Barnes Ice Cap in the Southern Canadian Arctic as study region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_aux_data_path = os.path.join(\"..\", \"data\", \"tutorials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_file_path = os.path.join(tutorial_aux_data_path, \"arcticdem_mosaic_100m_v4.1_dem__excerpt_barnes-ice-cap.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load barnes ice cap outline\n",
    "barnes_shp = gpd.read_feather(os.path.join(tutorial_aux_data_path, \"barnes_ice_cap.feather\")).unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, CryoSat-2 SARIn ground tracks, that pass in some defined period\n",
    "close to Barnes Ice Cap, will be loaded. You can best reduce the\n",
    "downloaded and processed amount of data, by editing the start and end\n",
    "dates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryoswath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m misc  \u001b[38;5;66;03m# this module contains helper functions\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdateutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelativedelta\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m relativedelta\n",
      "File \u001b[0;32m~/Documents/PhD/projects/2023__cryoswath/scripts/../cryoswath/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gis, misc, l1b, l2, l3, l4, test_plots\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmisc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_plots\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# subpackage\u001b[39;00m\n\u001b[1;32m      6\u001b[0m            ]\n",
      "File \u001b[0;32m~/Documents/PhD/projects/2023__cryoswath/scripts/../cryoswath/gis.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ! tbi:\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/projects/2023__cryoswath/scripts/../cryoswath/misc.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m covariance, linear_model, preprocessing\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NaturalNameWarning\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Software/miniforge3/envs/cryoswath/lib/python3.12/site-packages/sklearn/utils/validation.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _object_dtype_isnan\n\u001b[1;32m     31\u001b[0m FLOAT_DTYPES \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat16)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# make the package available/from search path\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from cryoswath import misc  # this module contains helper functions\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_tracks = misc.load_cs_ground_tracks(\n",
    "    barnes_shp, \"2020-09\", \"2023-09\", # request region and period\n",
    "    buffer_period_by=relativedelta(months=1), # include previous and following months\n",
    "    buffer_region_by=5_000) # include basin margins\n",
    "print(\"number of tracks:\", example_tracks.shape[0], \"\\n last five:\", example_tracks.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pandas.Series` contains the CryoSat-2 tracks. However, what we are\n",
    "more interested in, here, is the Index of the Series which can be used\n",
    "to download the associated L1b files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You could as well immediately pass the region definitions to the\n",
    "download function - in fact, you could jump all the steps below. This\n",
    "detour is only for the purpose of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check whether L1b files are present and download if they aren't\n",
    "# l1b.download_wrapper(track_idx=example_tracks.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level L1b to L2 processing\n",
    "\n",
    "L1b describes a data stage where the data have undergone some\n",
    "preprocessing but relate only indirectly to the observable world.\n",
    "\n",
    "L2 refers to data that describe physical \"real-world\" properties. This\n",
    "will mainly be the surface elevation at various locations, here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first of the tracks.\n",
    "\n",
    "You can load the L1b data by either using the index of the concerned\n",
    "track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = l1b.L1bData.from_id(example_tracks.index[0], drop_outside=barnes_shp, smooth_phase_difference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, first groups of continuous samples have to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.tag_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the groups identified, you can \"unwrap\" the phase difference.\n",
    "(Unwrapping is like realizing that a series of time stamps: 11 pm, 12\n",
    "pm, 1 am extends over two days.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.unwrap_phase_diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, you can have the reference elevations calculated for each\n",
    "apparent echo origin. For this, obviously, one need a reference\n",
    "elevation dataset. For this tutorial, it is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_file_path = os.path.join(tutorial_aux_data_path, \"arcticdem_mosaic_100m_v4.1_dem__excerpt_barnes-ice-cap.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_ambiguous_reference_elevation(dem_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cryoswath frequently uses the elevation difference of the echo origin\n",
    "with regard to the reference dataset. Below, this value is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_elev_diff_to_ref()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each group, the best fitting of the ambiguous echo origins\n",
    "is found based on a statistical measure. Below, the default is made\n",
    "explicit. As long as you prefer the default statistic, you don't need to\n",
    "pass the argument. However, it shows how you could define any custom\n",
    "measure you deem appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation\n",
    "def the_default(elev_diff):\n",
    "    return np.argmin(np.abs(np.median(elev_diff, axis=0))**2+median_abs_deviation(elev_diff, axis=0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_l1b_data = example_l1b_data.append_best_fit_phase_index(best_column=the_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the result:  \n",
    "*(The points in different shades of gray indicate other mathematically\n",
    "valid echo origins.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath.test_plots.waveform import dem_transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_transect(example_l1b_data.isel(time_20_ku=[len(example_l1b_data.time_20_ku)//2]), dem_file_name_or_path=dem_file_path, selected_phase_only=False)#, ax=ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final remarks L1b to L2**\n",
    "\n",
    "This were the most important steps in the L1b to L2 processing. Usually,\n",
    "you will rather use shorthands to have the steps done above\n",
    "automatically. However, if you have reasons to do so, this is how you\n",
    "can trigger all the major steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 to L3 processing\n",
    "\n",
    "L3 data features slots for a set of locations and time steps. For our\n",
    "case, L3 data refers to gridded monthly elevation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, all the tracks previously are processed - now, using a shorthand.\n",
    "\n",
    "The arguments, usually, are not necessary. Here, they set the caching\n",
    "location to the tutorial directory, confine the processed data to the\n",
    "study region, pass the elevation model, set a threshold for the maximum\n",
    "disagreement wrt. the reference DEM, prohibit parallelism (often a good\n",
    "idea on notebooks with limited working memory), define a coordinate\n",
    "reference system (CRS) for the output locations, and make explicit that\n",
    "all data is processed, even if it is present. The last option, you may\n",
    "want to disable if you know that the tracks have not been processed with\n",
    "different settings (e.g., you run the cell for the second time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:* should you do the l1b to l2 processing below in multiple\n",
    "        sessions (takes about 30 min in total), the returned\n",
    "        `swath_data, poca_data` will only contain the part processed in\n",
    "        the last run. Should you want to load all data, use the\n",
    "        commented cell below (only works once you've run the first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "swath_data, poca_data = l2.from_id(example_tracks.index, cache_fullname=os.path.join(tmp_path, \"tmp_l2_cache\"), drop_outside=barnes_shp, dem_file_name_or_path=dem_file_path, max_elev_diff=150, cores=1, crs=3413, reprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes a few minutes (2-3 min on my notebook)\n",
    "# empty_shape = gpd.GeoSeries().union_all(\"coverage\")\n",
    "# swath_data, poca_data = l2.from_id(example_tracks.index, reprocess=False, \n",
    "#                                    save_or_return=\"return\", # only return data (do not save to disk)\n",
    "#                                    drop_outside=empty_shape, # indirectly: do not process any files\n",
    "#                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take a look at the data by producing histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "swath_data.h_diff.plot.hist(label=f\"swath, Σ={len(swath_data):_}\", bins=100, density=True)\n",
    "poca_data.h_diff.plot.hist(label=f\"POCA, Σ={len(poca_data):_}\", bins=100, density=True, ax=plt.gca(), alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aggregate the point data into a regular grid. You will\n",
    "recognize most of the arguments from the previous steps. However, I\n",
    "would like to highlight the `agg_func_and_meta` argument. This allows to\n",
    "pass a custom aggregation function. Be careful modifying the aggregates\n",
    "names, because as of now (Feb 2025) those names are hardcoded in other\n",
    "functions. So feel free to play around, but keep the names if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_iqr_cnt(data):\n",
    "    quartiles = np.quantile(data, [.25, .5, .75])\n",
    "    return pd.DataFrame([[\n",
    "        quartiles[1],  # the median\n",
    "        quartiles[2]-quartiles[0],  # the inter-quartile range\n",
    "        len(data)  # the data count\n",
    "    ]], columns=[\"_median\", \"_iqr\", \"_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a few minutes (3-4 min at 4 GHz)\n",
    "gridded_elev_diffs = l3.build_dataset(\n",
    "    barnes_shp, start_datetime=\"2020-09\", end_datetime=\"2023-09\",\n",
    "    cache_filename=\"tmp_l2_cache\",\n",
    "    drop_outside=barnes_shp,\n",
    "    buffer_region_by=5_000,\n",
    "    crs=3413,\n",
    "    reprocess=False,\n",
    "    timestep_months = 1,  # temporal resolution\n",
    "    window_ntimesteps = 3,  # aggregates multiple time steps (moving window)\n",
    "    spatial_res_meter = 500,  # size of the grid cells\n",
    "    # below, you need to disclose the returned format\n",
    "    agg_func_and_meta = (med_iqr_cnt, {\"_median\": \"f4\", \"_iqr\": \"f4\", \"_count\": \"i4\"}),\n",
    ").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you see warnings about dumped data in the output above, don't\n",
    "worry. This usually concerns only data outside of the glaciers and it\n",
    "can be recovered using a helper function that collects the dumps. I'll\n",
    "fix it (#37) after finishing the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 to L4 processing\n",
    "\n",
    "L4 data may contain inferred information. In our case, that concerns\n",
    "void filling and calculating change rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do a rudimentary data clean-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_elev_diffs[\"_median\"] = gridded_elev_diffs._median.where(gridded_elev_diffs._count>3).where(gridded_elev_diffs._iqr<30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, the elevation change rate can be fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoswath import l4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_fit_results = l4.fit_trend__seasons_removed(gridded_elev_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before later we can fill the voids, we should exclude outliers to\n",
    "prevent them from distorting the data distribution. Here, I do so by\n",
    "requiring the trend variance to be less than 2 m²/yr² and the amplitude\n",
    "variances of the annual and semi-annual cycle to be smaller than 100 m².\n",
    "This corresponds roughly to limiting the trend and amplitude 2-sigma\n",
    "uncertainties to 3 m/yr and 20 m, respectively.\n",
    "\n",
    "For clarity, below the new \"trend\" and \"trend_std\" variables are\n",
    "defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.logical_and(\n",
    "    gridded_fit_results.curvefit_covariance.sel(cov_i=\"trend\", cov_j=\"trend\") < 2,\n",
    "    np.logical_and(gridded_fit_results.curvefit_covariance.sel(cov_i=\"amp_yearly\", cov_j=\"amp_yearly\") < 100,\n",
    "                    gridded_fit_results.curvefit_covariance.sel(cov_i=\"amp_semiyr\", cov_j=\"amp_semiyr\") < 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = gridded_fit_results.where(mask).sel(param=\"trend\", cov_i=\"trend\", cov_j=\"trend\")\n",
    "gridded_trends[\"trend\"] = gridded_trends.curvefit_coefficients\n",
    "gridded_trends[\"trend_std\"] = gridded_trends.curvefit_covariance**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, then, ensure that there is no missing grid cells. This used to occur\n",
    "before I moved to Zarr for regions with larger gaps between glaciers. If\n",
    "grid cells are missing, this messes up the apparent resolution as seen\n",
    "by `rioxarray` that is used in the background.\n",
    "\n",
    "Further, we cut the data that is outside of the glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "barnes_GeoDataFrame_SRID3413 = gpd.read_feather(os.path.join(tutorial_aux_data_path, \"barnes_ice_cap.feather\")).to_crs(gridded_trends.rio.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.fill_missing_coords(gridded_trends, *barnes_GeoDataFrame_SRID3413.total_bounds)\n",
    "gridded_trends = gridded_trends.rio.clip(barnes_GeoDataFrame_SRID3413.make_valid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we append the elevation of a reference DEM and assign the grid\n",
    "cells to basins. Actually, here we do *not* consider the basins, but\n",
    "treat the entire ice cap collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.append_elevation_reference(gridded_trends.rio.clip(barnes_GeoDataFrame_SRID3413.geometry), dem_file_name_or_path=dem_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends = l3.fill_voids(gridded_trends[[\"trend\", \"trend_std\", \"ref_elev\"]],\n",
    "                               main_var=\"trend\",\n",
    "                               error=\"trend_std\",\n",
    "                               elev=\"ref_elev\",\n",
    "                               basin_shapes=barnes_GeoDataFrame_SRID3413,\n",
    "                               per=tuple(), outlier_replace=True, outlier_limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_trends.trend.T.plot(robust=True, cmap=\"RdYlBu\")\n",
    "plt.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regarding the above figure*  \n",
    "There are artifacts, showing as patches of strong elevation gains and\n",
    "losses (blue and red) at higher elevations along the ice divide at the\n",
    "center of the ice cap. You could use the `dem_transect` plotting\n",
    "function from above to investigate their origin. Some issues are known\n",
    "to especially affect data covering only less than 4 years and to occur\n",
    "around summits (web-search: systematic error cryosat swath)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series\n",
    "\n",
    "**coming soon**\n",
    "\n",
    "Calculating time series works in analogy to the above. However, this\n",
    "tutorial works on a RGI \"complex\", here, the Barnes Ice Cap, while I\n",
    "usually work on \"basins\", i.e., constituting parts of complexes. I need\n",
    "to adapt the code a bit to work on complexes as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cryoswath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
